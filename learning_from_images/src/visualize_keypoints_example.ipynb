{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example image prediction with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(image, boxes, keypoints):\n",
    "    \"\"\"\n",
    "    Visualize the output model on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The original image.\n",
    "    boxes (numpy.ndarray): Array of bounding boxes, each box is [x, y, width, height].\n",
    "    keypoints (numpy.ndarray): Array of keypoints for pose estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    result_image = image.copy()\n",
    "    \n",
    "    # Font setting for the numbers\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, box in enumerate(boxes):\n",
    "        center_x, center_y, width, height = box\n",
    "        top_left_x = int(center_x - width / 2)\n",
    "        top_left_y = int(center_y - height / 2)\n",
    "        bottom_right_x = int(top_left_x + width)\n",
    "        bottom_right_y = int(top_left_y + height)\n",
    "\n",
    "        # Draw the box\n",
    "        #cv2.rectangle(result_image, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (0, 255, 0), 2)\n",
    "        \n",
    "        # # Write the number of the box\n",
    "        # text = str(i)\n",
    "        # (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "        # text_offset_x = top_left_x + text_width\n",
    "        # text_offset_y = top_left_y + text_height\n",
    "        # box_coords = ((text_offset_x, text_offset_y - text_height), (top_left_x, text_offset_y + 4))\n",
    "        # cv2.rectangle(result_image, box_coords[0], box_coords[1], (255, 255, 255), cv2.FILLED)\n",
    "        # cv2.putText(result_image, text, (top_left_x, text_offset_y), font, font_scale, (0, 0, 255), font_thickness)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in keypoints:\n",
    "        for x, y in keypoint:\n",
    "            cv2.circle(result_image, (int(x), int(y)), 20, (255, 0, 0), -1)\n",
    "    \n",
    "    return result_image\n",
    "\n",
    "def plot_results(image, mode=\"inline\", scale=1):\n",
    "    \"\"\"\n",
    "    Displays an image using either a popup window or inline display.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy.ndarray or CV2 image\n",
    "        The image to display.\n",
    "    mode : str, optional\n",
    "        The display mode. Must be either \"popup\" or \"inline\". If \"popup\", the image is displayed in a popup window. If \"inline\", the image is displayed inline in the notebook. Default is \"inline\".\n",
    "    scale : float, optional\n",
    "        The scaling factor for the image. Default is 1.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the mode is not \"popup\" or \"inline\".\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == \"popup\":\n",
    "        pass\n",
    "    elif mode == \"inline\":\n",
    "        if type(image) == np.ndarray:\n",
    "            # Convert from BGR to RGB (because OpenCV uses BGR order for color channels, whereas PIL uses RGB.)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image_rgb)\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be either 'popup' or 'inline'\")\n",
    "\n",
    "    if type(image) is np.ndarray:\n",
    "        cv2.imshow('Image', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        width, height = image.size\n",
    "        display(image.resize((int(width*scale), int(height*scale))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the model\n",
    "image = cv2.imread('example_images/yellow/yellow3.jpg') \n",
    "\n",
    "results = model(image)\n",
    "\n",
    "boxes = results[0].boxes.xywh  # Boxes object for bbox outputs\n",
    "keypoints = results[0].keypoints.xy  # Keypoints object for pose outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results)\n",
    "keypoints_all = results[0].keypoints\n",
    "#print(keypoints_3d)\n",
    "print(keypoints_all.data)\n",
    "print(keypoints_all.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = visualize_output(image, boxes, keypoints)\n",
    "plot_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image for the mockup:\n",
    "cv2.imwrite('ari_yellow3_kps.jpg', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ergonomic_pose.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
