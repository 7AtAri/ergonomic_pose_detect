{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergonomic pose fine-tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning yolo for our classification task\n",
    "\n",
    "- option 1: freeze the complete model, use it as a feature extractor and add a classification head to be trained on our dataset\n",
    "- option 2: cut the keypoint output layer, freeze the rest of the model and add a classification head to be trained on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) YOLOv8pose model as feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping bei hoher num_epochs (300)\n",
    "# batch_size festlegen (8, 16?)\n",
    "\n",
    "# HPO:\n",
    "# momentum: [0.9, 0.95]\n",
    "# lr: [1e-4, 5e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#%pip install -U ultralytics\n",
    "import torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strangely the yolo model does not like to be frozen and put to eval mode (then it loads some own dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had to outcomment this + yolomodel.eval() later\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False # freeze the YOLO model\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "    #print(f\"{name} is {'not ' if param.requires_grad else ''}frozen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# `feature_extractor` is the part of the model suitable for feature extraction\n",
    "\n",
    "class YOLO_kp_Classifier(nn.Module):\n",
    "    def __init__(self, num_keypoints, num_classes=3):\n",
    "        super(YOLO_kp_Classifier, self).__init__()\n",
    "        \n",
    "        # add classification layer to the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_keypoints*2, 512),  # 17 keypoints * 2 (x, y coordinates for each keypoint)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_flattened):\n",
    "        \n",
    "        output = self.classifier(keypoints_flattened)\n",
    "        return output\n",
    "\n",
    "class YOLO_KP_Scorer(nn.Module):\n",
    "    def __init__(self, num_keypoints, h1, h2):\n",
    "        super(YOLO_KP_Scorer, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(num_keypoints*2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(h2, h2 // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(h2 // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_flattened):\n",
    "        x = self.layer1(keypoints_flattened)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        output = self.dense(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier model\n",
    "num_classes = 7  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model\n",
    "\n",
    "kp_classifier_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be applied to the keypoints not the images I think - Vipin\n",
    "\n",
    "# Custom transform because YOLO likes Numpy Arrays / CV2 Images,\n",
    "# it automatically resizes it to its preffered size, and converts\n",
    "# the output back to the source size, better for precision and normalization\n",
    "from sympy import im\n",
    "\n",
    "\n",
    "class PILToNumpyTransform:\n",
    "    def __call__(self, pil_img):\n",
    "        \"\"\"\n",
    "        Convert a PIL Image to an OpenCV Image / Numpy Array.\n",
    "\n",
    "        Parameters:\n",
    "            pil_img (PIL.Image): The PIL Image to be converted.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The converted OpenCV Image in RGB format.\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to NumPy array (RGB)\n",
    "        img_array = np.array(pil_img)\n",
    "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img_array\n",
    "    \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((640, 480)), # Resize images to 640x640\n",
    "    transforms.ToTensor() # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/Users/Vipin\\Documents/BHT/3. Semester/Learning from images/Pose Dataset Complete'\n",
    "own_dataset = datasets.ImageFolder(root=data_path, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 118\n",
       "    Root location: E:/Users/Vipin\\Documents/BHT/3. Semester/Learning from images/Pose Dataset Complete\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(640, 480), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data into train, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(13)  # set seed for reproducibility of the split\n",
    "train_and_val_dataset, test_dataset = random_split(own_dataset, [0.8, 0.2], generator=generator1)  # 80% training and evaluation, 20% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction with yolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_from_yolo(model, inputs):\n",
    "    with torch.no_grad(): # gradients are not computed for the frozen model\n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            result = model(input.unsqueeze(0))[0].keypoints.xy[0].to(input.device)\n",
    "            print(result.shape)\n",
    "            if result.shape[0] < 17:\n",
    "                result = torch.cat([result, torch.zeros(17 - result.shape[0], 2).to(input.device)], dim = 0)\n",
    "            \n",
    "            results.append(result)\n",
    "    return torch.stack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check if this is sufficient, otherwise try z-score normalization or other methods\n",
    "\n",
    "# todo: !!!!!!!!! check correct position to use it / get min max earlier !!!!!!!!! needs to be the same normalization for all the data\n",
    "\n",
    "def process_keypoints_for_classifier(keypoints):\n",
    "    # Calculate mean and standard deviation with keepdim=True to preserve broadcasting compatibility\n",
    "    mean_vals = keypoints.mean(dim=1, keepdim=True)\n",
    "    std_vals = keypoints.std(dim=1, keepdim=True) + 1e-6  # Adding a small epsilon to prevent division by zero\n",
    "\n",
    "    # Perform standardization\n",
    "    kp_tensor_standardized = (keypoints - mean_vals) / std_vals\n",
    "    \n",
    "    # Flatten the last two dimensions while keeping the batch dimension\n",
    "    batch_flattened = kp_tensor_standardized.view(keypoints.size(0), -1)\n",
    "    \n",
    "    return batch_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the model:\n",
    "\n",
    "- with cross-entropy loss function (fits our classification task)\n",
    "- Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#%pip install scikit-learn\n",
    "#%pip install optuna\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier model\n",
    "num_classes = 7  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(yolo_model, dataset, batch_size, device):\n",
    "    processed_keypoints_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Assuming `dataset` is an iterable of (input, label) pairs\n",
    "    for inputs, labels in dataset:\n",
    "        inputs = inputs.to(device)\n",
    "        keypoints = get_keypoints_from_yolo(yolo_model, inputs)  # Extract keypoints        \n",
    "        processed_kps = process_keypoints_for_classifier(keypoints)  # Process keypoints        \n",
    "        processed_keypoints_list.append(processed_kps)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    processed_keypoints_tensor = torch.cat(processed_keypoints_list, dim=0)\n",
    "    labels_tensor = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    # Create a new TensorDataset and DataLoader\n",
    "    preprocessed_dataset = TensorDataset(processed_keypoints_tensor, labels_tensor)\n",
    "    preprocessed_loader = DataLoader(preprocessed_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return preprocessed_loader\n",
    "    \n",
    "def train_and_eval_model(kp_classifier_model, yolo_model, optimizer, loss_fn, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    kp_classifier_model = kp_classifier_model.to(device)  # move model to device\n",
    "    yolo_model = yolo_model.to(device)  # move YOLO model to device\n",
    "    kp_classifier_model.train()  # set model to training mode\n",
    "    \n",
    "    print(\"type of train_loader\", type(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            # access images and labels\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            #print(\"inputs\", type(inputs), len(inputs), inputs)\n",
    "            #inputs, labels = inputs.to(device), labels.to(device)  # move data to device\n",
    "            \n",
    "            # keypoints = get_keypoints_from_yolo(yolo_model, inputs) # get keypoints from the YOLO model\n",
    "            # processed_kps = process_keypoints_for_classifier(keypoints) # prepare the keypoints for the classifier\n",
    "            #print(\"keypoints\", type(keypoints), len(keypoints[0]), keypoints)\n",
    "            #print(\"processed_kps\", type(processed_kps), len(processed_kps[0]), processed_kps)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            classification_output = kp_classifier_model(inputs) # get results for the classification\n",
    "            loss = loss_fn(classification_output.float(), labels.float())  # Compute loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "        \n",
    "        kp_classifier_model.eval()  # model to evaluation mode\n",
    "\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "            for inputs, labels in val_loader:  # iterate over validation dataset\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # move data to device\n",
    "                \n",
    "                # Not necessary because already preprocessed\n",
    "                # keypoints = get_keypoints_from_yolo(yolo_model, inputs) # get keypoints from the YOLO model\n",
    "                # processed_kps = process_keypoints_for_classifier(keypoints) # prepare the keypoints for the classifier\n",
    "                \n",
    "                classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "                loss = loss_fn(classification_output.float() , labels.float())  # compute loss\n",
    "                total_loss += loss.item()  # accumulate the loss\n",
    "                # get predictions for output\n",
    "                _, predicted = torch.max(classification_output.data, 1)\n",
    "                # collect the predictions and labels\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            # calculate the validation metrics:\n",
    "            avg_loss = total_loss / len(val_loader)  # get the average loss\n",
    "            # accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "            # f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "            # conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "            \n",
    "            print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "            # print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            # print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "            # print(\"Confusion Matrix:\")\n",
    "            # print(conf_matrix)\n",
    "            \n",
    "            # here: F1 score chosen as the metric to optimize\n",
    "            # other options: - combining metrics like accuracy and F1 score to maximize on both\n",
    "            #                        - or multi-objective optimization on F1 score and accuracy\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was generated by gpt-4\n",
    "\n",
    "def get_k_fold_indices(n, k=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generate indices for k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of samples in the dataset.\n",
    "    - k: Number of folds.\n",
    "    - random_seed: Optional seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples, each containing (train_indices, val_indices) for a fold.\n",
    "    \"\"\"\n",
    "    # Initialize the random generator\n",
    "    g = torch.Generator()\n",
    "    if random_seed is not None:\n",
    "        g.manual_seed(random_seed)\n",
    "    \n",
    "    # Generate a random permutation of indices\n",
    "    indices = torch.randperm(n, generator=g).tolist()\n",
    "    \n",
    "    # Calculate fold sizes\n",
    "    fold_sizes = [n // k for _ in range(k)]\n",
    "    for i in range(n % k):\n",
    "        fold_sizes[i] += 1\n",
    "    \n",
    "    # Generate train and validation indices for each fold\n",
    "    current = 0\n",
    "    folds_indices = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, end = current, current + fold_size\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = indices[:start] + indices[end:]\n",
    "        folds_indices.append((train_indices, val_indices))\n",
    "        current = end\n",
    "    \n",
    "    return folds_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, dataset, yolo_model):\n",
    "    # Define hyperparameters to optimize\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-3, 1e-4, 5e-5])\n",
    "    h1 = trial.suggest_categorical(\"h1\", [256, 512, 1024])\n",
    "    h2 = trial.suggest_categorical(\"h2\", [256, 512, 1024])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    num_epochs = trial.suggest_categorical(\"num_epochs\", [100, 200, 300])\n",
    "\n",
    "    validation_scores = []\n",
    "\n",
    "    n = len(dataset)\n",
    "    k = 5\n",
    "    folds_indices = get_k_fold_indices(n, k, random_seed=13)\n",
    "    device = get_device()\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(folds_indices, start=1):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        print(\"type of train subset: \" , type(train_subset))\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        train_loader = preprocess_dataset(yolo_model, train_loader, batch_size, device)\n",
    "        val_loader = preprocess_dataset(yolo_model, val_loader, batch_size, device)\n",
    "        print(\"type of train loader: \" , type(train_loader))\n",
    "        # initializing the model and optimizer with the chosen hyperparameters\n",
    "        kp_classifier_model = YOLO_KP_Scorer(num_keypoints, h1, h2).to(device)\n",
    "        optimizer = torch.optim.Adam(kp_classifier_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "        # training and evaluating the model\n",
    "        validation_score = train_and_eval_model(kp_classifier_model, yolo_model, optimizer, nn.MSELoss(), num_epochs, train_loader, val_loader, device)\n",
    "        validation_scores.append(validation_score)\n",
    "\n",
    "    return np.mean(validation_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolov8n-pose.pt')\n",
    "optimize = partial(objective, dataset = train_and_val_dataset, yolo_model = yolo_model)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(optimize)  # run 20 trials\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.001, 'h1': 256, 'h2': 256, 'batch_size': 16, 'num_epochs': 200}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "# extract best_parameters\n",
    "batch_size = best_params['batch_size']\n",
    "lr = best_params['lr']\n",
    "h1 = best_params['h1']\n",
    "h2 = best_params['h2']\n",
    "num_epochs = best_params['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train and evaluate final model on test set:\n",
    "\n",
    "Unfortunately Optuna cannot output the best model, so we train again on the combined train and validation set with the best parameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader for final evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 3 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.0ms\n",
      "Speed: 0.0ms preprocess, 11.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.5ms preprocess, 10.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 12.7ms\n",
      "Speed: 0.0ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.5ms\n",
      "Speed: 0.0ms preprocess, 11.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 2 persons, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.0ms\n",
      "Speed: 0.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.0ms\n",
      "Speed: 0.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 2 persons, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.5ms\n",
      "Speed: 0.0ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 11.5ms\n",
      "Speed: 0.0ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.3ms preprocess, 8.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 3 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.3ms\n",
      "Speed: 0.0ms preprocess, 9.3ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 32.0ms\n",
      "Speed: 0.0ms preprocess, 32.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.5ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 37.5ms\n",
      "Speed: 0.0ms preprocess, 37.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n"
     ]
    }
   ],
   "source": [
    "train_and_eval_loader = DataLoader(train_and_val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "train_and_eval_loader = preprocess_dataset(yolo_model, train_and_eval_loader, batch_size, get_device())\n",
    "test_loader = preprocess_dataset(yolo_model, test_loader, batch_size, get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final evaluation on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Vipin\\Documents\\BHT\\3. Semester\\Learning from images\\ergonomic_pose_detect\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 12.5707\n",
      "Epoch [2/200], Loss: 10.4433\n",
      "Epoch [3/200], Loss: 10.3179\n",
      "Epoch [4/200], Loss: 6.1191\n",
      "Epoch [5/200], Loss: 11.1890\n",
      "Epoch [6/200], Loss: 9.9254\n",
      "Epoch [7/200], Loss: 13.9532\n",
      "Epoch [8/200], Loss: 7.4578\n",
      "Epoch [9/200], Loss: 7.8521\n",
      "Epoch [10/200], Loss: 13.2084\n",
      "Epoch [11/200], Loss: 12.0688\n",
      "Epoch [12/200], Loss: 12.9167\n",
      "Epoch [13/200], Loss: 15.8881\n",
      "Epoch [14/200], Loss: 13.3051\n",
      "Epoch [15/200], Loss: 15.8472\n",
      "Epoch [16/200], Loss: 18.4688\n",
      "Epoch [17/200], Loss: 11.9625\n",
      "Epoch [18/200], Loss: 11.0522\n",
      "Epoch [19/200], Loss: 11.6126\n",
      "Epoch [20/200], Loss: 14.8053\n",
      "Epoch [21/200], Loss: 12.6833\n",
      "Epoch [22/200], Loss: 9.9276\n",
      "Epoch [23/200], Loss: 10.0348\n",
      "Epoch [24/200], Loss: 14.6187\n",
      "Epoch [25/200], Loss: 12.0907\n",
      "Epoch [26/200], Loss: 14.6944\n",
      "Epoch [27/200], Loss: 9.5206\n",
      "Epoch [28/200], Loss: 15.4482\n",
      "Epoch [29/200], Loss: 12.1048\n",
      "Epoch [30/200], Loss: 15.6549\n",
      "Epoch [31/200], Loss: 12.3722\n",
      "Epoch [32/200], Loss: 13.1254\n",
      "Epoch [33/200], Loss: 12.6422\n",
      "Epoch [34/200], Loss: 11.9839\n",
      "Epoch [35/200], Loss: 15.9747\n",
      "Epoch [36/200], Loss: 10.9212\n",
      "Epoch [37/200], Loss: 13.9864\n",
      "Epoch [38/200], Loss: 14.6952\n",
      "Epoch [39/200], Loss: 11.8380\n",
      "Epoch [40/200], Loss: 12.2610\n",
      "Epoch [41/200], Loss: 10.9269\n",
      "Epoch [42/200], Loss: 10.4400\n",
      "Epoch [43/200], Loss: 12.7091\n",
      "Epoch [44/200], Loss: 12.4495\n",
      "Epoch [45/200], Loss: 10.6662\n",
      "Epoch [46/200], Loss: 8.5232\n",
      "Epoch [47/200], Loss: 11.6238\n",
      "Epoch [48/200], Loss: 12.4377\n",
      "Epoch [49/200], Loss: 13.7912\n",
      "Epoch [50/200], Loss: 10.5274\n",
      "Epoch [51/200], Loss: 14.1465\n",
      "Epoch [52/200], Loss: 8.6062\n",
      "Epoch [53/200], Loss: 17.5899\n",
      "Epoch [54/200], Loss: 10.8554\n",
      "Epoch [55/200], Loss: 10.3289\n",
      "Epoch [56/200], Loss: 13.1104\n",
      "Epoch [57/200], Loss: 13.3382\n",
      "Epoch [58/200], Loss: 10.5305\n",
      "Epoch [59/200], Loss: 9.1378\n",
      "Epoch [60/200], Loss: 10.1958\n",
      "Epoch [61/200], Loss: 8.2418\n",
      "Epoch [62/200], Loss: 14.0971\n",
      "Epoch [63/200], Loss: 11.1883\n",
      "Epoch [64/200], Loss: 9.6220\n",
      "Epoch [65/200], Loss: 15.6953\n",
      "Epoch [66/200], Loss: 12.7541\n",
      "Epoch [67/200], Loss: 13.5533\n",
      "Epoch [68/200], Loss: 11.3760\n",
      "Epoch [69/200], Loss: 9.9359\n",
      "Epoch [70/200], Loss: 13.0037\n",
      "Epoch [71/200], Loss: 15.3970\n",
      "Epoch [72/200], Loss: 21.8610\n",
      "Epoch [73/200], Loss: 13.3119\n",
      "Epoch [74/200], Loss: 16.0582\n",
      "Epoch [75/200], Loss: 15.0869\n",
      "Epoch [76/200], Loss: 7.6864\n",
      "Epoch [77/200], Loss: 12.0222\n",
      "Epoch [78/200], Loss: 10.2506\n",
      "Epoch [79/200], Loss: 12.4471\n",
      "Epoch [80/200], Loss: 17.9303\n",
      "Epoch [81/200], Loss: 7.5603\n",
      "Epoch [82/200], Loss: 9.3057\n",
      "Epoch [83/200], Loss: 5.2361\n",
      "Epoch [84/200], Loss: 11.4582\n",
      "Epoch [85/200], Loss: 11.6128\n",
      "Epoch [86/200], Loss: 12.5022\n",
      "Epoch [87/200], Loss: 8.8590\n",
      "Epoch [88/200], Loss: 12.7974\n",
      "Epoch [89/200], Loss: 14.8610\n",
      "Epoch [90/200], Loss: 17.9370\n",
      "Epoch [91/200], Loss: 9.6775\n",
      "Epoch [92/200], Loss: 9.5756\n",
      "Epoch [93/200], Loss: 8.2993\n",
      "Epoch [94/200], Loss: 13.4172\n",
      "Epoch [95/200], Loss: 10.9726\n",
      "Epoch [96/200], Loss: 9.9401\n",
      "Epoch [97/200], Loss: 13.2492\n",
      "Epoch [98/200], Loss: 10.7577\n",
      "Epoch [99/200], Loss: 14.0758\n",
      "Epoch [100/200], Loss: 3.9229\n",
      "Epoch [101/200], Loss: 12.5790\n",
      "Epoch [102/200], Loss: 13.0755\n",
      "Epoch [103/200], Loss: 10.7283\n",
      "Epoch [104/200], Loss: 15.9879\n",
      "Epoch [105/200], Loss: 9.6330\n",
      "Epoch [106/200], Loss: 11.5143\n",
      "Epoch [107/200], Loss: 7.8482\n",
      "Epoch [108/200], Loss: 13.1333\n",
      "Epoch [109/200], Loss: 9.9943\n",
      "Epoch [110/200], Loss: 9.6531\n",
      "Epoch [111/200], Loss: 12.9869\n",
      "Epoch [112/200], Loss: 8.9777\n",
      "Epoch [113/200], Loss: 12.5014\n",
      "Epoch [114/200], Loss: 9.2443\n",
      "Epoch [115/200], Loss: 12.0227\n",
      "Epoch [116/200], Loss: 9.4676\n",
      "Epoch [117/200], Loss: 13.3409\n",
      "Epoch [118/200], Loss: 7.0367\n",
      "Epoch [119/200], Loss: 5.3938\n",
      "Epoch [120/200], Loss: 12.2619\n",
      "Epoch [121/200], Loss: 11.6320\n",
      "Epoch [122/200], Loss: 12.3535\n",
      "Epoch [123/200], Loss: 11.4422\n",
      "Epoch [124/200], Loss: 15.9018\n",
      "Epoch [125/200], Loss: 11.2004\n",
      "Epoch [126/200], Loss: 9.9224\n",
      "Epoch [127/200], Loss: 13.2297\n",
      "Epoch [128/200], Loss: 14.1044\n",
      "Epoch [129/200], Loss: 13.2651\n",
      "Epoch [130/200], Loss: 12.3721\n",
      "Epoch [131/200], Loss: 12.0986\n",
      "Epoch [132/200], Loss: 11.2068\n",
      "Epoch [133/200], Loss: 9.4265\n",
      "Epoch [134/200], Loss: 13.0542\n",
      "Epoch [135/200], Loss: 7.1894\n",
      "Epoch [136/200], Loss: 12.5970\n",
      "Epoch [137/200], Loss: 11.2821\n",
      "Epoch [138/200], Loss: 7.4573\n",
      "Epoch [139/200], Loss: 12.2698\n",
      "Epoch [140/200], Loss: 6.7521\n",
      "Epoch [141/200], Loss: 9.2786\n",
      "Epoch [142/200], Loss: 7.9173\n",
      "Epoch [143/200], Loss: 11.8804\n",
      "Epoch [144/200], Loss: 14.6940\n",
      "Epoch [145/200], Loss: 10.1204\n",
      "Epoch [146/200], Loss: 13.5886\n",
      "Epoch [147/200], Loss: 10.2609\n",
      "Epoch [148/200], Loss: 13.7074\n",
      "Epoch [149/200], Loss: 12.8639\n",
      "Epoch [150/200], Loss: 6.8213\n",
      "Epoch [151/200], Loss: 12.8526\n",
      "Epoch [152/200], Loss: 10.7555\n",
      "Epoch [153/200], Loss: 11.3088\n",
      "Epoch [154/200], Loss: 15.6906\n",
      "Epoch [155/200], Loss: 12.1640\n",
      "Epoch [156/200], Loss: 12.0333\n",
      "Epoch [157/200], Loss: 13.1815\n",
      "Epoch [158/200], Loss: 11.2832\n",
      "Epoch [159/200], Loss: 12.6708\n",
      "Epoch [160/200], Loss: 15.8139\n",
      "Epoch [161/200], Loss: 18.2422\n",
      "Epoch [162/200], Loss: 9.4634\n",
      "Epoch [163/200], Loss: 14.8382\n",
      "Epoch [164/200], Loss: 15.3428\n",
      "Epoch [165/200], Loss: 11.6785\n",
      "Epoch [166/200], Loss: 11.5319\n",
      "Epoch [167/200], Loss: 15.0343\n",
      "Epoch [168/200], Loss: 10.5513\n",
      "Epoch [169/200], Loss: 14.2905\n",
      "Epoch [170/200], Loss: 9.8684\n",
      "Epoch [171/200], Loss: 7.8831\n",
      "Epoch [172/200], Loss: 12.9253\n",
      "Epoch [173/200], Loss: 12.4373\n",
      "Epoch [174/200], Loss: 9.0598\n",
      "Epoch [175/200], Loss: 16.0146\n",
      "Epoch [176/200], Loss: 8.2941\n",
      "Epoch [177/200], Loss: 12.4937\n",
      "Epoch [178/200], Loss: 13.6171\n",
      "Epoch [179/200], Loss: 10.1124\n",
      "Epoch [180/200], Loss: 13.4562\n",
      "Epoch [181/200], Loss: 7.6999\n",
      "Epoch [182/200], Loss: 13.2069\n",
      "Epoch [183/200], Loss: 10.8707\n",
      "Epoch [184/200], Loss: 10.2687\n",
      "Epoch [185/200], Loss: 8.6192\n",
      "Epoch [186/200], Loss: 8.1607\n",
      "Epoch [187/200], Loss: 15.6113\n",
      "Epoch [188/200], Loss: 11.4169\n",
      "Epoch [189/200], Loss: 13.7467\n",
      "Epoch [190/200], Loss: 13.6216\n",
      "Epoch [191/200], Loss: 12.5496\n",
      "Epoch [192/200], Loss: 9.2742\n",
      "Epoch [193/200], Loss: 15.5389\n",
      "Epoch [194/200], Loss: 16.7446\n",
      "Epoch [195/200], Loss: 11.1684\n",
      "Epoch [196/200], Loss: 15.0610\n",
      "Epoch [197/200], Loss: 12.0916\n",
      "Epoch [198/200], Loss: 9.8581\n",
      "Epoch [199/200], Loss: 9.6662\n",
      "Epoch [200/200], Loss: 14.4674\n",
      "Test Loss: 13.3496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Vipin\\Documents\\BHT\\3. Semester\\Learning from images\\ergonomic_pose_detect\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(kp_classifier_model.classifier.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# new instance of the model:\n",
    "kp_classifier_model = YOLO_KP_Scorer(17, h1, h2)\n",
    "kp_classifier_model = kp_classifier_model.to(get_device())  # move model to device\n",
    "kp_classifier_model.train()  # set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_and_eval_loader:  # Assuming data_loader is your DataLoader instance for the dataset\n",
    "        inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "        loss = criterion(classification_output.float() , labels.float())  # Compute loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "kp_classifier_model.eval()  # model to evaluation mode\n",
    "\n",
    "total_loss = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "    for inputs, labels in test_loader:  # iterate over validation dataset\n",
    "        inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "        classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "        loss = criterion(classification_output.float(), labels.float())  # compute loss\n",
    "        total_loss += loss.item()  # accumulate the loss\n",
    "        _, predicted = torch.max(classification_output.data, 1)\n",
    "        # collect the predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    # calculate the validation metrics:\n",
    "    avg_loss = total_loss / len(test_loader)  # get the average loss\n",
    "    # accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "    # f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "    # conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
