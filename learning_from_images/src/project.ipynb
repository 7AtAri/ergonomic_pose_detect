{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_output(image, boxes, keypoints):\n",
    "    \"\"\"\n",
    "    Visualize the output model on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy.ndarray): The original image.\n",
    "    boxes (numpy.ndarray): Array of bounding boxes, each box is [x, y, width, height].\n",
    "    keypoints (numpy.ndarray): Array of keypoints for pose estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    result_image = image.copy()\n",
    "    \n",
    "    # Font setting for the numbers\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, box in enumerate(boxes):\n",
    "        center_x, center_y, width, height = box\n",
    "        top_left_x = int(center_x - width / 2)\n",
    "        top_left_y = int(center_y - height / 2)\n",
    "        bottom_right_x = int(top_left_x + width)\n",
    "        bottom_right_y = int(top_left_y + height)\n",
    "\n",
    "        # Draw the box\n",
    "        #cv2.rectangle(result_image, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), (0, 255, 0), 2)\n",
    "        \n",
    "        # # Write the number of the box\n",
    "        # text = str(i)\n",
    "        # (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "        # text_offset_x = top_left_x + text_width\n",
    "        # text_offset_y = top_left_y + text_height\n",
    "        # box_coords = ((text_offset_x, text_offset_y - text_height), (top_left_x, text_offset_y + 4))\n",
    "        # cv2.rectangle(result_image, box_coords[0], box_coords[1], (255, 255, 255), cv2.FILLED)\n",
    "        # cv2.putText(result_image, text, (top_left_x, text_offset_y), font, font_scale, (0, 0, 255), font_thickness)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in keypoints:\n",
    "        for x, y in keypoint:\n",
    "            cv2.circle(result_image, (int(x), int(y)), 20, (255, 0, 0), -1)\n",
    "    \n",
    "    return result_image\n",
    "\n",
    "def plot_results(image, mode=\"inline\", scale=1):\n",
    "    \"\"\"\n",
    "    Displays an image using either a popup window or inline display.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy.ndarray or CV2 image\n",
    "        The image to display.\n",
    "    mode : str, optional\n",
    "        The display mode. Must be either \"popup\" or \"inline\". If \"popup\", the image is displayed in a popup window. If \"inline\", the image is displayed inline in the notebook. Default is \"inline\".\n",
    "    scale : float, optional\n",
    "        The scaling factor for the image. Default is 1.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the mode is not \"popup\" or \"inline\".\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == \"popup\":\n",
    "        pass\n",
    "    elif mode == \"inline\":\n",
    "        if type(image) == np.ndarray:\n",
    "            # Convert from BGR to RGB (because OpenCV uses BGR order for color channels, whereas PIL uses RGB.)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image_rgb)\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be either 'popup' or 'inline'\")\n",
    "\n",
    "    if type(image) is np.ndarray:\n",
    "        cv2.imshow('Image', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        width, height = image.size\n",
    "        display(image.resize((int(width*scale), int(height*scale))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small example image prediction with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 person, 78.1ms\n",
      "Speed: 2.9ms preprocess, 78.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "image = cv2.imread('example_images/yellow3.jpg') \n",
    "\n",
    "results = model(image)\n",
    "\n",
    "boxes = results[0].boxes.xywh  # Boxes object for bbox outputs\n",
    "keypoints = results[0].keypoints.xy  # Keypoints object for pose outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the keypoints are also calculated in 3D which we will leverage for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.2903e+03, 1.4783e+03, 9.9350e-01],\n",
      "         [1.3883e+03, 1.3790e+03, 9.9267e-01],\n",
      "         [1.2600e+03, 1.3812e+03, 7.7818e-01],\n",
      "         [1.7325e+03, 1.3233e+03, 9.6801e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2132e-02],\n",
      "         [1.9563e+03, 1.7039e+03, 9.9153e-01],\n",
      "         [1.6893e+03, 1.5877e+03, 9.9206e-01],\n",
      "         [1.5165e+03, 2.2164e+03, 9.7732e-01],\n",
      "         [1.2452e+03, 1.8650e+03, 9.7607e-01],\n",
      "         [1.3291e+03, 2.2291e+03, 9.8469e-01],\n",
      "         [8.3613e+02, 2.0155e+03, 9.8376e-01],\n",
      "         [2.4183e+03, 2.8356e+03, 9.8898e-01],\n",
      "         [2.2288e+03, 2.7412e+03, 9.9286e-01],\n",
      "         [1.5074e+03, 2.8463e+03, 9.8288e-01],\n",
      "         [1.4614e+03, 2.7262e+03, 9.8562e-01],\n",
      "         [1.2421e+03, 3.3962e+03, 9.0765e-01],\n",
      "         [1.3356e+03, 3.2737e+03, 9.3313e-01]]])\n",
      "torch.Size([1, 17, 3])\n"
     ]
    }
   ],
   "source": [
    "#print(results)\n",
    "keypoints_all = results[0].keypoints\n",
    "#print(keypoints_3d)\n",
    "print(keypoints_all.data)\n",
    "print(keypoints_all.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = visualize_output(image, boxes, keypoints)\n",
    "# plot_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the image for the mockup:\n",
    "cv2.imwrite('ari_yellow3_kps.jpg', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning yolo for our classification task\n",
    "\n",
    "- option 1: freeze the complete model, use it as a feature extractor and add a classification head to be trained on our dataset\n",
    "- option 2: cut the keypoint output layer, freeze the rest of the model and add a classification head to be trained on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) using yolo as feature extractor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freezing the yolo model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## freezing the model by setting the requires_grad attribute to False:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# `feature_extractor` is the part of the model suitable for feature extraction\n",
    "\n",
    "class YOLO_kp_Classifier(nn.Module):\n",
    "    def __init__(self, num_keypoints, num_classes=3):\n",
    "        super(YOLO_kp_Classifier, self).__init__()\n",
    "        # to flatten the output \n",
    "        self.flatten = nn.Flatten()\n",
    "        # add new classification layer(s) to the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_keypoints* 3, 512),  # 17 keypoints * 3 (x, y, z coordinates for each keypoint)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_3d):\n",
    "        keypoints_3d = self.flatten(keypoints_3d)\n",
    "        output = self.classifier(keypoints_3d)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add cross-entropy loss function (fits our classification task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier\n",
    "num_classes = 3  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model\n",
    "\n",
    "kp_classifier_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(kp_classifier_model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()  # YOLO model set to evaluation mode because we are not training it\n",
    "\n",
    "with torch.no_grad():  # gradients are not computed for the frozen model\n",
    "    results = model(input)\n",
    "    keypoints = results[0].keypoints.data\n",
    "    # do we need to adjust the shape?\n",
    "    \n",
    "    #keypoints = keypoints.view(1, -1)  # Reshape keypoints for the classifier, if necessary\n",
    "    classification_output = kp_classifier_model(keypoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! add dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo add dataloader!!!\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 50 # check for appropriate number of epochs\n",
    "dataloader = None  # todo: replace with our own dataloader\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
