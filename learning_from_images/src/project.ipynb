{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergonomic pose fine-tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning yolo for our classification task\n",
    "\n",
    "- option 1: freeze the complete model, use it as a feature extractor and add a classification head to be trained on our dataset\n",
    "- option 2: cut the keypoint output layer, freeze the rest of the model and add a classification head to be trained on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) YOLOv8pose model as feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping bei hoher num_epochs (300)\n",
    "# batch_size festlegen (8, 16?)\n",
    "\n",
    "# HPO:\n",
    "# momentum: [0.9, 0.95]\n",
    "# lr: [1e-4, 5e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#%pip install -U ultralytics\n",
    "import torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strangely the yolo model does not like to be frozen and put to eval mode (then it loads some own dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had to outcomment this + yolomodel.eval() later\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False # freeze the YOLO model\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "    #print(f\"{name} is {'not ' if param.requires_grad else ''}frozen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# `feature_extractor` is the part of the model suitable for feature extraction\n",
    "\n",
    "class YOLO_kp_Classifier(nn.Module):\n",
    "    def __init__(self, num_keypoints, num_classes=3):\n",
    "        super(YOLO_kp_Classifier, self).__init__()\n",
    "        \n",
    "        # add classification layer to the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_keypoints*2, 512),  # 17 keypoints * 2 (x, y coordinates for each keypoint)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_flattened):\n",
    "        \n",
    "        output = self.classifier(keypoints_flattened)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier model\n",
    "num_classes = 7  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model\n",
    "\n",
    "kp_classifier_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be applied to the keypoints not the images I think - Vipin\n",
    "\n",
    "# Custom transform because YOLO likes Numpy Arrays / CV2 Images,\n",
    "# it automatically resizes it to its preffered size, and converts\n",
    "# the output back to the source size, better for precision and normalization\n",
    "from sympy import im\n",
    "\n",
    "\n",
    "class PILToNumpyTransform:\n",
    "    def __call__(self, pil_img):\n",
    "        \"\"\"\n",
    "        Convert a PIL Image to an OpenCV Image / Numpy Array.\n",
    "\n",
    "        Parameters:\n",
    "            pil_img (PIL.Image): The PIL Image to be converted.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The converted OpenCV Image in RGB format.\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to NumPy array (RGB)\n",
    "        img_array = np.array(pil_img)\n",
    "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img_array\n",
    "    \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((640, 480)), # Resize images to 640x640\n",
    "    transforms.ToTensor() # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/Users/Vipin\\Documents/BHT/3. Semester/Learning from images/Pose Dataset Complete'\n",
    "own_dataset = datasets.ImageFolder(root=data_path, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data into train, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(13)  # set seed for reproducibility of the split\n",
    "train_and_val_dataset, test_dataset = random_split(own_dataset, [0.8, 0.2], generator=generator1)  # 80% training and evaluation, 20% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction with yolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_from_yolo(model, inputs):\n",
    "    with torch.no_grad(): # gradients are not computed for the frozen model\n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            result = model(input.unsqueeze(0))[0].keypoints.xy[0].to(input.device)\n",
    "            print(result.shape)\n",
    "            if result.shape[0] < 17:\n",
    "                result = torch.cat([result, torch.zeros(17 - result.shape[0], 2).to(input.device)], dim = 0)\n",
    "            \n",
    "            results.append(result)\n",
    "    return torch.stack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check if this is sufficient, otherwise try z-score normalization or other methods\n",
    "\n",
    "# todo: !!!!!!!!! check correct position to use it / get min max earlier !!!!!!!!! needs to be the same normalization for all the data\n",
    "\n",
    "def process_keypoints_for_classifier(keypoints):\n",
    "    # Calculate mean and standard deviation with keepdim=True to preserve broadcasting compatibility\n",
    "    mean_vals = keypoints.mean(dim=1, keepdim=True)\n",
    "    std_vals = keypoints.std(dim=1, keepdim=True) + 1e-6  # Adding a small epsilon to prevent division by zero\n",
    "\n",
    "    # Perform standardization\n",
    "    kp_tensor_standardized = (keypoints - mean_vals) / std_vals\n",
    "    \n",
    "    # Flatten the last two dimensions while keeping the batch dimension\n",
    "    batch_flattened = kp_tensor_standardized.view(keypoints.size(0), -1)\n",
    "    \n",
    "    return batch_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the model:\n",
    "\n",
    "- with cross-entropy loss function (fits our classification task)\n",
    "- Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#%pip install scikit-learn\n",
    "#%pip install optuna\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier model\n",
    "num_classes = 7  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(yolo_model, dataset, batch_size, device):\n",
    "    processed_keypoints_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Assuming `dataset` is an iterable of (input, label) pairs\n",
    "    for inputs, labels in dataset:\n",
    "        inputs = inputs.to(device)\n",
    "        keypoints = get_keypoints_from_yolo(yolo_model, inputs)  # Extract keypoints        \n",
    "        processed_kps = process_keypoints_for_classifier(keypoints)  # Process keypoints        \n",
    "        processed_keypoints_list.append(processed_kps)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    processed_keypoints_tensor = torch.cat(processed_keypoints_list, dim=0)\n",
    "    labels_tensor = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    # Create a new TensorDataset and DataLoader\n",
    "    preprocessed_dataset = TensorDataset(processed_keypoints_tensor, labels_tensor)\n",
    "    preprocessed_loader = DataLoader(preprocessed_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return preprocessed_loader\n",
    "    \n",
    "def train_and_eval_model(kp_classifier_model, yolo_model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    kp_classifier_model = kp_classifier_model.to(device)  # move model to device\n",
    "    yolo_model = yolo_model.to(device)  # move YOLO model to device\n",
    "    kp_classifier_model.train()  # set model to training mode\n",
    "    \n",
    "    print(\"type of train_loader\", type(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            # access images and labels\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            #print(\"inputs\", type(inputs), len(inputs), inputs)\n",
    "            #inputs, labels = inputs.to(device), labels.to(device)  # move data to device\n",
    "            \n",
    "            # keypoints = get_keypoints_from_yolo(yolo_model, inputs) # get keypoints from the YOLO model\n",
    "            # processed_kps = process_keypoints_for_classifier(keypoints) # prepare the keypoints for the classifier\n",
    "            #print(\"keypoints\", type(keypoints), len(keypoints[0]), keypoints)\n",
    "            #print(\"processed_kps\", type(processed_kps), len(processed_kps[0]), processed_kps)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            classification_output = kp_classifier_model(inputs) # get results for the classification\n",
    "            loss = criterion(classification_output, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "        \n",
    "        kp_classifier_model.eval()  # model to evaluation mode\n",
    "\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "            for inputs, labels in val_loader:  # iterate over validation dataset\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # move data to device\n",
    "                \n",
    "                # Not necessary because already preprocessed\n",
    "                # keypoints = get_keypoints_from_yolo(yolo_model, inputs) # get keypoints from the YOLO model\n",
    "                # processed_kps = process_keypoints_for_classifier(keypoints) # prepare the keypoints for the classifier\n",
    "                \n",
    "                classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "                loss = criterion(classification_output , labels)  # compute loss\n",
    "                total_loss += loss.item()  # accumulate the loss\n",
    "                # get predictions for output\n",
    "                _, predicted = torch.max(classification_output.data, 1)\n",
    "                # collect the predictions and labels\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            # calculate the validation metrics:\n",
    "            avg_loss = total_loss / len(val_loader)  # get the average loss\n",
    "            accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "            f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "            conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "            \n",
    "            print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(conf_matrix)\n",
    "            \n",
    "            # here: F1 score chosen as the metric to optimize\n",
    "            # other options: - combining metrics like accuracy and F1 score to maximize on both\n",
    "            #                        - or multi-objective optimization on F1 score and accuracy\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was generated by gpt-4\n",
    "\n",
    "def get_k_fold_indices(n, k=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generate indices for k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of samples in the dataset.\n",
    "    - k: Number of folds.\n",
    "    - random_seed: Optional seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples, each containing (train_indices, val_indices) for a fold.\n",
    "    \"\"\"\n",
    "    # Initialize the random generator\n",
    "    g = torch.Generator()\n",
    "    if random_seed is not None:\n",
    "        g.manual_seed(random_seed)\n",
    "    \n",
    "    # Generate a random permutation of indices\n",
    "    indices = torch.randperm(n, generator=g).tolist()\n",
    "    \n",
    "    # Calculate fold sizes\n",
    "    fold_sizes = [n // k for _ in range(k)]\n",
    "    for i in range(n % k):\n",
    "        fold_sizes[i] += 1\n",
    "    \n",
    "    # Generate train and validation indices for each fold\n",
    "    current = 0\n",
    "    folds_indices = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, end = current, current + fold_size\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = indices[:start] + indices[end:]\n",
    "        folds_indices.append((train_indices, val_indices))\n",
    "        current = end\n",
    "    \n",
    "    return folds_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, dataset, yolo_model):\n",
    "    # Define hyperparameters to optimize\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-4, 5e-5]) #, 5e-4, 1e-3])\n",
    "    #momentum = trial.suggest_categorical(\"momentum\", [0.9, 0.95])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    num_epochs = trial.suggest_categorical(\"num_epochs\", [100])\n",
    "\n",
    "    validation_scores = []\n",
    "\n",
    "    n = len(dataset)\n",
    "    k = 5\n",
    "    folds_indices = get_k_fold_indices(n, k, random_seed=13)\n",
    "    device = get_device()\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(folds_indices, start=1):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        print(\"type of train subset: \" , type(train_subset))\n",
    "        train_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        train_loader = preprocess_dataset(yolo_model, train_loader, batch_size, device)\n",
    "        val_loader = preprocess_dataset(yolo_model, val_loader, batch_size, device)\n",
    "        print(\"type of train loader: \" , type(train_loader))\n",
    "        # initializing the model and optimizer with the chosen hyperparameters\n",
    "        kp_classifier_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes)\n",
    "        optimizer = torch.optim.Adam(kp_classifier_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "        # training and evaluating the model\n",
    "        validation_score = train_and_eval_model(kp_classifier_model, yolo_model, optimizer, num_epochs, train_loader, val_loader, device)\n",
    "        validation_scores.append(validation_score)\n",
    "\n",
    "    return np.mean(validation_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolov8n-pose.pt')\n",
    "optimize = partial(objective, dataset = train_and_val_dataset, yolo_model = yolo_model)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize, n_trials=20)  # run 20 trials\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "# extract best_parameters\n",
    "batch_size = best_params['batch_size']\n",
    "lr = best_params['lr']\n",
    "# momentum = best_params['momentum']\n",
    "num_epochs = best_params['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train and evaluate final model on test set:\n",
    "\n",
    "Unfortunately Optuna cannot output the best model, so we train again on the combined train and validation set with the best parameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader for final evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval_loader = DataLoader(train_and_val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "train_and_eval_loader = preprocess_dataset(yolo_model, train_and_eval_loader, batch_size, get_device())\n",
    "test_loader = preprocess_dataset(yolo_model, test_loader, batch_size, get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final evaluation on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(kp_classifier_model.classifier.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# new instance of the model:\n",
    "kp_classifier_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes)\n",
    "kp_classifier_model = kp_classifier_model.to(get_device())  # move model to device\n",
    "kp_classifier_model.train()  # set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_and_eval_loader:  # Assuming data_loader is your DataLoader instance for the dataset\n",
    "        inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "        loss = criterion(classification_output , labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "    kp_classifier_model.eval()  # model to evaluation mode\n",
    "\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "        for inputs, labels in test_loader:  # iterate over validation dataset\n",
    "            inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "            classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "            loss = criterion(classification_output , labels)  # compute loss\n",
    "            total_loss += loss.item()  # accumulate the loss\n",
    "            _, predicted = torch.max(classification_output.data, 1)\n",
    "            # collect the predictions and labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        # calculate the validation metrics:\n",
    "        avg_loss = total_loss / len(test_loader)  # get the average loss\n",
    "        accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "        \n",
    "        print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
