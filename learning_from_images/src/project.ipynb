{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergonomic pose fine-tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning yolo for our classification task\n",
    "\n",
    "- option 1: freeze the complete model, use it as a feature extractor and add a classification head to be trained on our dataset\n",
    "- option 2: cut the keypoint output layer, freeze the rest of the model and add a classification head to be trained on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) YOLOv8pose model as feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping bei hoher num_epochs (300)\n",
    "# batch_size festlegen (8, 16?)\n",
    "\n",
    "# HPO:\n",
    "# momentum: [0.9, 0.95]\n",
    "# lr: [1e-4, 5e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strangely the yolo model does not like to be frozen and put to eval mode (then it loads some own dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had to outcomment this + yolomodel.eval() later\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False # freeze the YOLO model\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "    #print(f\"{name} is {'not ' if param.requires_grad else ''}frozen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# `feature_extractor` is the part of the model suitable for feature extraction\n",
    "\n",
    "class YOLO_kp_Classifier(nn.Module):\n",
    "    def __init__(self, num_keypoints, num_classes, h1, h2):\n",
    "        super(YOLO_kp_Classifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(num_keypoints*2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(h2, h2 // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(h2 // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_flattened):\n",
    "        x = self.layer1(keypoints_flattened)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        output = self.dense(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class YOLO_KP_Scorer(nn.Module):\n",
    "    def __init__(self, num_keypoints, h1, h2):\n",
    "        super(YOLO_KP_Scorer, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(num_keypoints*2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(h2, h2 // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(h2 // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, keypoints_flattened):\n",
    "        x = self.layer1(keypoints_flattened)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        output = self.dense(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier model\n",
    "num_classes = 3  #  3-class classification problem\n",
    "num_keypoints = 17  # 17 keypoints in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be applied to the keypoints not the images I think - Vipin\n",
    "\n",
    "# Custom transform because YOLO likes Numpy Arrays / CV2 Images,\n",
    "# it automatically resizes it to its preffered size, and converts\n",
    "# the output back to the source size, better for precision and normalization\n",
    "from sympy import im\n",
    "\n",
    "\n",
    "class PILToNumpyTransform:\n",
    "    def __call__(self, pil_img):\n",
    "        \"\"\"\n",
    "        Convert a PIL Image to an OpenCV Image / Numpy Array.\n",
    "\n",
    "        Parameters:\n",
    "            pil_img (PIL.Image): The PIL Image to be converted.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The converted OpenCV Image in RGB format.\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to NumPy array (RGB)\n",
    "        img_array = np.array(pil_img)\n",
    "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img_array\n",
    "    \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((640, 480)), # Resize images to 640x640\n",
    "    transforms.ToTensor() # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/Users/Vipin\\Documents/BHT/3. Semester/Learning from images/Pose Dataset_binned'\n",
    "own_dataset = datasets.ImageFolder(root=data_path, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 118\n",
       "    Root location: E:/Users/Vipin\\Documents/BHT/3. Semester/Learning from images/Pose Dataset_binned\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(640, 480), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data into train, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(13)  # set seed for reproducibility of the split\n",
    "train_and_val_dataset, test_dataset = random_split(own_dataset, [0.8, 0.2], generator=generator1)  # 80% training and evaluation, 20% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction with yolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_from_yolo(model, inputs):\n",
    "    with torch.no_grad(): # gradients are not computed for the frozen model\n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            result = model(input.unsqueeze(0))[0].keypoints.xy[0].to(input.device)\n",
    "            print(result.shape)\n",
    "            if result.shape[0] < 17:\n",
    "                result = torch.cat([result, torch.zeros(17 - result.shape[0], 2).to(input.device)], dim = 0)\n",
    "            \n",
    "            results.append(result)\n",
    "    return torch.stack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-Value\n",
    "def process_keypoints_for_classifier(keypoints):\n",
    "    # Calculate mean and standard deviation with keepdim=True to preserve broadcasting compatibility\n",
    "    mean_vals = keypoints.mean(dim=1, keepdim=True)\n",
    "    std_vals = keypoints.std(dim=1, keepdim=True) + 1e-6  # Adding a small epsilon to prevent division by zero\n",
    "\n",
    "    # Perform standardization\n",
    "    kp_tensor_standardized = (keypoints - mean_vals) / std_vals\n",
    "    \n",
    "    # Flatten the last two dimensions while keeping the batch dimension\n",
    "    batch_flattened = kp_tensor_standardized.view(keypoints.size(0), -1)\n",
    "    \n",
    "    return batch_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the model:\n",
    "\n",
    "- with cross-entropy loss function (fits our classification task)\n",
    "- Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(yolo_model, dataset, batch_size, device):\n",
    "    processed_keypoints_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Assuming `dataset` is an iterable of (input, label) pairs\n",
    "    for inputs, labels in dataset:\n",
    "        inputs = inputs.to(device)\n",
    "        keypoints = get_keypoints_from_yolo(yolo_model, inputs)  # Extract keypoints        \n",
    "        processed_kps = process_keypoints_for_classifier(keypoints)  # Process keypoints        \n",
    "        processed_keypoints_list.append(processed_kps)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    processed_keypoints_tensor = torch.cat(processed_keypoints_list, dim=0)\n",
    "    labels_tensor = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    # Create a new TensorDataset and DataLoader\n",
    "    preprocessed_dataset = TensorDataset(processed_keypoints_tensor, labels_tensor)\n",
    "    preprocessed_loader = DataLoader(preprocessed_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return preprocessed_loader\n",
    "    \n",
    "def train_and_eval_model(kp_classifier_model, yolo_model, optimizer, loss_fn, num_epochs, train_loader, val_loader, device):\n",
    "    \n",
    "    kp_classifier_model = kp_classifier_model.to(device)  # move model to device\n",
    "    yolo_model = yolo_model.to(device)  # move YOLO model to device\n",
    "    kp_classifier_model.train()  # set model to training mode\n",
    "    \n",
    "    print(\"type of train_loader\", type(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            # access images and labels\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            classification_output = kp_classifier_model(inputs) # get results for the classification\n",
    "            \n",
    "            # Use this for classification\n",
    "            loss = loss_fn(classification_output, labels)\n",
    "            \n",
    "            # Use this for scoring\n",
    "            # loss = loss_fn(classification_output.float(), labels.float())\n",
    "            \n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "        \n",
    "        kp_classifier_model.eval()  # model to evaluation mode\n",
    "\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "            for inputs, labels in val_loader:  # iterate over validation dataset\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # move data to device\n",
    "                \n",
    "                # Not necessary because already preprocessed\n",
    "                # keypoints = get_keypoints_from_yolo(yolo_model, inputs) # get keypoints from the YOLO model\n",
    "                # processed_kps = process_keypoints_for_classifier(keypoints) # prepare the keypoints for the classifier\n",
    "                \n",
    "                classification_output = kp_classifier_model(inputs) # get results for the classification \n",
    "                \n",
    "                # Use this for classification\n",
    "                loss = loss_fn(classification_output, labels)\n",
    "                \n",
    "                # Use this for scoring\n",
    "                # loss = loss_fn(classification_output.float(), labels.float())\n",
    "                \n",
    "                total_loss += loss.item()  # accumulate the loss\n",
    "                # get predictions for output\n",
    "                _, predicted = torch.max(classification_output.data, 1)\n",
    "                # collect the predictions and labels\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            # calculate the validation metrics:\n",
    "            avg_loss = total_loss / len(val_loader)  # get the average loss\n",
    "            accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "            f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "            conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "            \n",
    "            print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(conf_matrix)\n",
    "            \n",
    "            # here: F1 score chosen as the metric to optimize\n",
    "            # other options: - combining metrics like accuracy and F1 score to maximize on both\n",
    "            #                        - or multi-objective optimization on F1 score and accuracy\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was generated by gpt-4\n",
    "\n",
    "def get_k_fold_indices(n, k=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generate indices for k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of samples in the dataset.\n",
    "    - k: Number of folds.\n",
    "    - random_seed: Optional seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples, each containing (train_indices, val_indices) for a fold.\n",
    "    \"\"\"\n",
    "    # Initialize the random generator\n",
    "    g = torch.Generator()\n",
    "    if random_seed is not None:\n",
    "        g.manual_seed(random_seed)\n",
    "    \n",
    "    # Generate a random permutation of indices\n",
    "    indices = torch.randperm(n, generator=g).tolist()\n",
    "    \n",
    "    # Calculate fold sizes\n",
    "    fold_sizes = [n // k for _ in range(k)]\n",
    "    for i in range(n % k):\n",
    "        fold_sizes[i] += 1\n",
    "    \n",
    "    # Generate train and validation indices for each fold\n",
    "    current = 0\n",
    "    folds_indices = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, end = current, current + fold_size\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = indices[:start] + indices[end:]\n",
    "        folds_indices.append((train_indices, val_indices))\n",
    "        current = end\n",
    "    \n",
    "    return folds_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, dataset, yolo_model):\n",
    "    # Define hyperparameters to optimize\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-3, 1e-4, 5e-5])\n",
    "    h1 = trial.suggest_categorical(\"h1\", [256, 512, 1024])\n",
    "    h2 = trial.suggest_categorical(\"h2\", [256, 512, 1024])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    num_epochs = trial.suggest_categorical(\"num_epochs\", [100, 200, 300])\n",
    "\n",
    "    validation_scores = []\n",
    "\n",
    "    n = len(dataset)\n",
    "    k = 5\n",
    "    folds_indices = get_k_fold_indices(n, k, random_seed=13)\n",
    "    device = get_device()\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(folds_indices, start=1):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        print(\"type of train subset: \" , type(train_subset))\n",
    "        \n",
    "        # Create data loaders for the iteration in preprocessing with yolo\n",
    "        # Batch size 1 because preprocessing must be done image by image to extract the correct keypoints\n",
    "        train_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Batch size goes here\n",
    "        train_loader = preprocess_dataset(yolo_model, train_loader, batch_size, device)\n",
    "        val_loader = preprocess_dataset(yolo_model, val_loader, batch_size, device)\n",
    "        print(\"type of train loader: \" , type(train_loader))\n",
    "        \n",
    "        # Initializing the model and optimizer with the chosen hyperparameters\n",
    "        # kp_model = YOLO_KP_Scorer(num_keypoints, h1, h2).to(device)\n",
    "        kp_model = YOLO_kp_Classifier(num_keypoints=num_keypoints, num_classes=num_classes, h1=h1, h2=h2).to(device)\n",
    "        optimizer = torch.optim.Adam(kp_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "        # training and evaluating the model - Watch out for the Loss - CrossEntropyLoss for classification and MSELoss for scoring\n",
    "        validation_score = train_and_eval_model(kp_model, yolo_model, optimizer, nn.CrossEntropyLoss(), num_epochs, train_loader, val_loader, device)\n",
    "        validation_scores.append(validation_score)\n",
    "\n",
    "    return np.mean(validation_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolov8n-pose.pt')\n",
    "optimize = partial(objective, dataset = train_and_val_dataset, yolo_model = yolo_model)\n",
    "search_space = {\n",
    "    \"lr\": [1e-3, 1e-4, 5e-5],\n",
    "    \"h1\": [256, 512, 1024],\n",
    "    \"h2\": [256, 512, 1024],\n",
    "    \"batch_size\": [4, 8, 16],\n",
    "    \"num_epochs\": [100, 200, 300]\n",
    "}\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction='maximize')\n",
    "study.optimize(optimize, n_trials=243)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0001, 'h1': 512, 'h2': 1024, 'batch_size': 4, 'num_epochs': 300}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "# extract best_parameters\n",
    "batch_size = best_params['batch_size']\n",
    "lr = best_params['lr']\n",
    "h1 = best_params['h1']\n",
    "h2 = best_params['h2']\n",
    "num_epochs = best_params['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train and evaluate final model on test set:\n",
    "\n",
    "Unfortunately Optuna cannot output the best model, so we train again on the combined train and validation set with the best parameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader for final evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 (no detections), 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.5ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 3 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.9ms\n",
      "Speed: 0.0ms preprocess, 6.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.5ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 9.0ms\n",
      "Speed: 0.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 (no detections), 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([0, 2])\n",
      "\n",
      "0: 640x480 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 3 persons, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 2 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n",
      "\n",
      "0: 640x480 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "torch.Size([17, 2])\n"
     ]
    }
   ],
   "source": [
    "train_and_eval_loader = DataLoader(train_and_val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "train_and_eval_loader = preprocess_dataset(yolo_model, train_and_eval_loader, batch_size, get_device())\n",
    "test_loader = preprocess_dataset(yolo_model, test_loader, batch_size, get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final evaluation on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 1.0472\n",
      "Epoch [2/300], Loss: 1.1228\n",
      "Epoch [3/300], Loss: 1.1898\n",
      "Epoch [4/300], Loss: 0.9125\n",
      "Epoch [5/300], Loss: 0.7267\n",
      "Epoch [6/300], Loss: 1.0057\n",
      "Epoch [7/300], Loss: 0.5941\n",
      "Epoch [8/300], Loss: 1.0955\n",
      "Epoch [9/300], Loss: 0.5706\n",
      "Epoch [10/300], Loss: 1.4644\n",
      "Epoch [11/300], Loss: 1.0247\n",
      "Epoch [12/300], Loss: 0.8891\n",
      "Epoch [13/300], Loss: 0.8012\n",
      "Epoch [14/300], Loss: 1.3533\n",
      "Epoch [15/300], Loss: 0.8050\n",
      "Epoch [16/300], Loss: 0.9901\n",
      "Epoch [17/300], Loss: 0.6548\n",
      "Epoch [18/300], Loss: 0.9349\n",
      "Epoch [19/300], Loss: 1.0869\n",
      "Epoch [20/300], Loss: 0.8145\n",
      "Epoch [21/300], Loss: 0.8749\n",
      "Epoch [22/300], Loss: 1.4428\n",
      "Epoch [23/300], Loss: 0.6674\n",
      "Epoch [24/300], Loss: 1.4872\n",
      "Epoch [25/300], Loss: 0.5583\n",
      "Epoch [26/300], Loss: 0.3842\n",
      "Epoch [27/300], Loss: 1.0396\n",
      "Epoch [28/300], Loss: 0.9086\n",
      "Epoch [29/300], Loss: 0.6882\n",
      "Epoch [30/300], Loss: 1.0397\n",
      "Epoch [31/300], Loss: 0.4248\n",
      "Epoch [32/300], Loss: 0.7491\n",
      "Epoch [33/300], Loss: 0.5157\n",
      "Epoch [34/300], Loss: 0.4379\n",
      "Epoch [35/300], Loss: 0.3722\n",
      "Epoch [36/300], Loss: 0.2188\n",
      "Epoch [37/300], Loss: 0.7004\n",
      "Epoch [38/300], Loss: 0.3831\n",
      "Epoch [39/300], Loss: 0.4100\n",
      "Epoch [40/300], Loss: 0.3460\n",
      "Epoch [41/300], Loss: 0.4683\n",
      "Epoch [42/300], Loss: 0.6450\n",
      "Epoch [43/300], Loss: 0.4539\n",
      "Epoch [44/300], Loss: 0.2540\n",
      "Epoch [45/300], Loss: 0.2313\n",
      "Epoch [46/300], Loss: 0.4489\n",
      "Epoch [47/300], Loss: 0.3539\n",
      "Epoch [48/300], Loss: 0.3276\n",
      "Epoch [49/300], Loss: 0.3295\n",
      "Epoch [50/300], Loss: 0.8637\n",
      "Epoch [51/300], Loss: 0.2218\n",
      "Epoch [52/300], Loss: 0.2048\n",
      "Epoch [53/300], Loss: 0.5868\n",
      "Epoch [54/300], Loss: 0.8842\n",
      "Epoch [55/300], Loss: 0.4726\n",
      "Epoch [56/300], Loss: 0.3307\n",
      "Epoch [57/300], Loss: 0.0753\n",
      "Epoch [58/300], Loss: 0.1549\n",
      "Epoch [59/300], Loss: 0.4892\n",
      "Epoch [60/300], Loss: 0.6712\n",
      "Epoch [61/300], Loss: 0.0171\n",
      "Epoch [62/300], Loss: 0.2056\n",
      "Epoch [63/300], Loss: 0.4905\n",
      "Epoch [64/300], Loss: 0.2140\n",
      "Epoch [65/300], Loss: 0.0859\n",
      "Epoch [66/300], Loss: 1.0481\n",
      "Epoch [67/300], Loss: 0.0832\n",
      "Epoch [68/300], Loss: 0.2248\n",
      "Epoch [69/300], Loss: 0.8847\n",
      "Epoch [70/300], Loss: 0.3559\n",
      "Epoch [71/300], Loss: 0.3505\n",
      "Epoch [72/300], Loss: 0.0357\n",
      "Epoch [73/300], Loss: 0.3311\n",
      "Epoch [74/300], Loss: 0.3581\n",
      "Epoch [75/300], Loss: 0.1623\n",
      "Epoch [76/300], Loss: 0.3014\n",
      "Epoch [77/300], Loss: 0.4732\n",
      "Epoch [78/300], Loss: 0.2641\n",
      "Epoch [79/300], Loss: 0.2390\n",
      "Epoch [80/300], Loss: 0.1847\n",
      "Epoch [81/300], Loss: 0.8449\n",
      "Epoch [82/300], Loss: 0.0467\n",
      "Epoch [83/300], Loss: 0.3362\n",
      "Epoch [84/300], Loss: 0.0767\n",
      "Epoch [85/300], Loss: 0.1790\n",
      "Epoch [86/300], Loss: 0.1777\n",
      "Epoch [87/300], Loss: 0.1278\n",
      "Epoch [88/300], Loss: 0.4455\n",
      "Epoch [89/300], Loss: 0.1030\n",
      "Epoch [90/300], Loss: 0.0495\n",
      "Epoch [91/300], Loss: 0.4380\n",
      "Epoch [92/300], Loss: 0.0209\n",
      "Epoch [93/300], Loss: 0.0390\n",
      "Epoch [94/300], Loss: 0.0998\n",
      "Epoch [95/300], Loss: 0.1867\n",
      "Epoch [96/300], Loss: 0.2221\n",
      "Epoch [97/300], Loss: 0.0514\n",
      "Epoch [98/300], Loss: 0.3205\n",
      "Epoch [99/300], Loss: 0.0372\n",
      "Epoch [100/300], Loss: 0.1291\n",
      "Epoch [101/300], Loss: 1.3061\n",
      "Epoch [102/300], Loss: 0.0889\n",
      "Epoch [103/300], Loss: 0.3481\n",
      "Epoch [104/300], Loss: 0.0049\n",
      "Epoch [105/300], Loss: 0.2036\n",
      "Epoch [106/300], Loss: 0.1639\n",
      "Epoch [107/300], Loss: 0.8600\n",
      "Epoch [108/300], Loss: 0.0000\n",
      "Epoch [109/300], Loss: 0.1194\n",
      "Epoch [110/300], Loss: 0.2868\n",
      "Epoch [111/300], Loss: 0.2378\n",
      "Epoch [112/300], Loss: 0.0108\n",
      "Epoch [113/300], Loss: 0.0222\n",
      "Epoch [114/300], Loss: 0.4048\n",
      "Epoch [115/300], Loss: 0.0806\n",
      "Epoch [116/300], Loss: 0.3177\n",
      "Epoch [117/300], Loss: 0.2054\n",
      "Epoch [118/300], Loss: 0.3391\n",
      "Epoch [119/300], Loss: 0.0214\n",
      "Epoch [120/300], Loss: 0.1186\n",
      "Epoch [121/300], Loss: 0.1587\n",
      "Epoch [122/300], Loss: 0.0512\n",
      "Epoch [123/300], Loss: 0.0081\n",
      "Epoch [124/300], Loss: 0.0913\n",
      "Epoch [125/300], Loss: 0.0201\n",
      "Epoch [126/300], Loss: 0.0134\n",
      "Epoch [127/300], Loss: 0.7824\n",
      "Epoch [128/300], Loss: 0.0879\n",
      "Epoch [129/300], Loss: 0.0062\n",
      "Epoch [130/300], Loss: 0.3131\n",
      "Epoch [131/300], Loss: 0.0018\n",
      "Epoch [132/300], Loss: 0.3666\n",
      "Epoch [133/300], Loss: 0.0015\n",
      "Epoch [134/300], Loss: 0.0074\n",
      "Epoch [135/300], Loss: 0.0103\n",
      "Epoch [136/300], Loss: 0.0416\n",
      "Epoch [137/300], Loss: 0.3090\n",
      "Epoch [138/300], Loss: 0.0094\n",
      "Epoch [139/300], Loss: 0.7286\n",
      "Epoch [140/300], Loss: 0.2456\n",
      "Epoch [141/300], Loss: 0.3937\n",
      "Epoch [142/300], Loss: 0.0013\n",
      "Epoch [143/300], Loss: 0.0281\n",
      "Epoch [144/300], Loss: 0.0186\n",
      "Epoch [145/300], Loss: 0.3763\n",
      "Epoch [146/300], Loss: 0.7827\n",
      "Epoch [147/300], Loss: 0.2875\n",
      "Epoch [148/300], Loss: 0.0095\n",
      "Epoch [149/300], Loss: 0.2059\n",
      "Epoch [150/300], Loss: 0.0018\n",
      "Epoch [151/300], Loss: 0.2010\n",
      "Epoch [152/300], Loss: 0.3368\n",
      "Epoch [153/300], Loss: 0.1570\n",
      "Epoch [154/300], Loss: 0.6424\n",
      "Epoch [155/300], Loss: 0.0317\n",
      "Epoch [156/300], Loss: 0.1750\n",
      "Epoch [157/300], Loss: 0.0135\n",
      "Epoch [158/300], Loss: 0.0032\n",
      "Epoch [159/300], Loss: 0.3259\n",
      "Epoch [160/300], Loss: 0.0253\n",
      "Epoch [161/300], Loss: 0.0135\n",
      "Epoch [162/300], Loss: 0.1651\n",
      "Epoch [163/300], Loss: 0.1187\n",
      "Epoch [164/300], Loss: 0.0001\n",
      "Epoch [165/300], Loss: 0.0027\n",
      "Epoch [166/300], Loss: 0.0376\n",
      "Epoch [167/300], Loss: 0.2208\n",
      "Epoch [168/300], Loss: 0.0077\n",
      "Epoch [169/300], Loss: 0.0278\n",
      "Epoch [170/300], Loss: 0.0003\n",
      "Epoch [171/300], Loss: 0.0081\n",
      "Epoch [172/300], Loss: 0.0077\n",
      "Epoch [173/300], Loss: 0.2691\n",
      "Epoch [174/300], Loss: 0.0038\n",
      "Epoch [175/300], Loss: 0.2070\n",
      "Epoch [176/300], Loss: 0.2666\n",
      "Epoch [177/300], Loss: 1.0995\n",
      "Epoch [178/300], Loss: 0.5772\n",
      "Epoch [179/300], Loss: 0.0315\n",
      "Epoch [180/300], Loss: 0.4535\n",
      "Epoch [181/300], Loss: 0.1021\n",
      "Epoch [182/300], Loss: 0.3711\n",
      "Epoch [183/300], Loss: 0.0102\n",
      "Epoch [184/300], Loss: 0.1304\n",
      "Epoch [185/300], Loss: 0.5251\n",
      "Epoch [186/300], Loss: 0.0024\n",
      "Epoch [187/300], Loss: 0.1675\n",
      "Epoch [188/300], Loss: 0.1631\n",
      "Epoch [189/300], Loss: 0.0132\n",
      "Epoch [190/300], Loss: 0.0133\n",
      "Epoch [191/300], Loss: 0.0188\n",
      "Epoch [192/300], Loss: 0.0004\n",
      "Epoch [193/300], Loss: 0.0010\n",
      "Epoch [194/300], Loss: 0.0088\n",
      "Epoch [195/300], Loss: 0.0013\n",
      "Epoch [196/300], Loss: 0.1195\n",
      "Epoch [197/300], Loss: 0.1890\n",
      "Epoch [198/300], Loss: 0.0650\n",
      "Epoch [199/300], Loss: 0.2331\n",
      "Epoch [200/300], Loss: 0.0448\n",
      "Epoch [201/300], Loss: 0.1768\n",
      "Epoch [202/300], Loss: 0.0005\n",
      "Epoch [203/300], Loss: 0.0705\n",
      "Epoch [204/300], Loss: 0.0007\n",
      "Epoch [205/300], Loss: 0.0003\n",
      "Epoch [206/300], Loss: 0.0122\n",
      "Epoch [207/300], Loss: 0.0064\n",
      "Epoch [208/300], Loss: 0.0247\n",
      "Epoch [209/300], Loss: 0.3481\n",
      "Epoch [210/300], Loss: 0.0021\n",
      "Epoch [211/300], Loss: 0.0005\n",
      "Epoch [212/300], Loss: 1.8757\n",
      "Epoch [213/300], Loss: 0.0477\n",
      "Epoch [214/300], Loss: 0.2370\n",
      "Epoch [215/300], Loss: 0.1668\n",
      "Epoch [216/300], Loss: 0.0151\n",
      "Epoch [217/300], Loss: 0.0954\n",
      "Epoch [218/300], Loss: 0.0904\n",
      "Epoch [219/300], Loss: 0.0057\n",
      "Epoch [220/300], Loss: 0.1935\n",
      "Epoch [221/300], Loss: 0.1341\n",
      "Epoch [222/300], Loss: 0.0001\n",
      "Epoch [223/300], Loss: 0.0046\n",
      "Epoch [224/300], Loss: 0.1442\n",
      "Epoch [225/300], Loss: 0.0501\n",
      "Epoch [226/300], Loss: 0.0378\n",
      "Epoch [227/300], Loss: 0.0013\n",
      "Epoch [228/300], Loss: 0.7861\n",
      "Epoch [229/300], Loss: 0.1022\n",
      "Epoch [230/300], Loss: 0.1596\n",
      "Epoch [231/300], Loss: 0.0419\n",
      "Epoch [232/300], Loss: 0.3083\n",
      "Epoch [233/300], Loss: 0.0005\n",
      "Epoch [234/300], Loss: 0.0009\n",
      "Epoch [235/300], Loss: 0.0039\n",
      "Epoch [236/300], Loss: 0.0035\n",
      "Epoch [237/300], Loss: 0.1504\n",
      "Epoch [238/300], Loss: 0.2343\n",
      "Epoch [239/300], Loss: 0.0023\n",
      "Epoch [240/300], Loss: 0.0273\n",
      "Epoch [241/300], Loss: 0.0427\n",
      "Epoch [242/300], Loss: 0.0010\n",
      "Epoch [243/300], Loss: 0.0671\n",
      "Epoch [244/300], Loss: 0.0011\n",
      "Epoch [245/300], Loss: 0.0006\n",
      "Epoch [246/300], Loss: 0.0078\n",
      "Epoch [247/300], Loss: 0.0076\n",
      "Epoch [248/300], Loss: 0.0068\n",
      "Epoch [249/300], Loss: 0.0959\n",
      "Epoch [250/300], Loss: 0.0072\n",
      "Epoch [251/300], Loss: 0.0274\n",
      "Epoch [252/300], Loss: 0.0453\n",
      "Epoch [253/300], Loss: 0.2246\n",
      "Epoch [254/300], Loss: 0.1583\n",
      "Epoch [255/300], Loss: 0.1999\n",
      "Epoch [256/300], Loss: 0.2615\n",
      "Epoch [257/300], Loss: 0.0232\n",
      "Epoch [258/300], Loss: 0.0175\n",
      "Epoch [259/300], Loss: 0.0065\n",
      "Epoch [260/300], Loss: 0.0054\n",
      "Epoch [261/300], Loss: 0.0013\n",
      "Epoch [262/300], Loss: 0.0010\n",
      "Epoch [263/300], Loss: 0.0122\n",
      "Epoch [264/300], Loss: 0.0008\n",
      "Epoch [265/300], Loss: 0.0038\n",
      "Epoch [266/300], Loss: 0.0214\n",
      "Epoch [267/300], Loss: 0.0023\n",
      "Epoch [268/300], Loss: 0.0620\n",
      "Epoch [269/300], Loss: 0.0003\n",
      "Epoch [270/300], Loss: 0.0219\n",
      "Epoch [271/300], Loss: 0.0005\n",
      "Epoch [272/300], Loss: 0.0006\n",
      "Epoch [273/300], Loss: 0.0001\n",
      "Epoch [274/300], Loss: 0.0023\n",
      "Epoch [275/300], Loss: 0.4860\n",
      "Epoch [276/300], Loss: 0.0048\n",
      "Epoch [277/300], Loss: 0.1150\n",
      "Epoch [278/300], Loss: 0.0002\n",
      "Epoch [279/300], Loss: 0.4094\n",
      "Epoch [280/300], Loss: 0.0100\n",
      "Epoch [281/300], Loss: 0.0011\n",
      "Epoch [282/300], Loss: 0.0008\n",
      "Epoch [283/300], Loss: 0.0019\n",
      "Epoch [284/300], Loss: 0.0026\n",
      "Epoch [285/300], Loss: 0.2762\n",
      "Epoch [286/300], Loss: 0.0015\n",
      "Epoch [287/300], Loss: 0.4787\n",
      "Epoch [288/300], Loss: 0.0233\n",
      "Epoch [289/300], Loss: 0.0030\n",
      "Epoch [290/300], Loss: 0.0000\n",
      "Epoch [291/300], Loss: 0.0004\n",
      "Epoch [292/300], Loss: 0.0000\n",
      "Epoch [293/300], Loss: 0.0008\n",
      "Epoch [294/300], Loss: 0.5593\n",
      "Epoch [295/300], Loss: 0.0003\n",
      "Epoch [296/300], Loss: 0.0061\n",
      "Epoch [297/300], Loss: 0.0006\n",
      "Epoch [298/300], Loss: 0.2639\n",
      "Epoch [299/300], Loss: 0.0000\n",
      "Epoch [300/300], Loss: 0.0084\n",
      "Test Loss: 4.0423\n",
      "Test Accuracy: 0.4783\n",
      "Test F1 Score: 0.4783\n",
      "Confusion Matrix:\n",
      "[[1 1 2]\n",
      " [3 5 2]\n",
      " [0 4 5]]\n"
     ]
    }
   ],
   "source": [
    "# new instance of the model:\n",
    "# kp_model = YOLO_KP_Scorer(17, h1, h2)\n",
    "kp_model = YOLO_kp_Classifier(num_keypoints, num_classes, h1, h2)\n",
    "kp_model = kp_model.to(get_device())  # move model to device\n",
    "kp_model.train()  # set model to training mode\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(kp_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_and_eval_loader:  # Assuming data_loader is your DataLoader instance for the dataset\n",
    "        inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        classification_output = kp_model(inputs) # get results for the classification \n",
    "        \n",
    "        # Use this for classification\n",
    "        loss = criterion(classification_output, labels)\n",
    "        \n",
    "        # Use this for scoring\n",
    "        # loss = criterion(classification_output.float(), labels.float())\n",
    "        \n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # todo: further processing, such as calculating accuracy or loss, goes here\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "kp_model.eval()  # model to evaluation mode\n",
    "\n",
    "total_loss = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():  # no need to compute gradients, because we are in evaluation mode\n",
    "    for inputs, labels in test_loader:  # iterate over validation dataset\n",
    "        inputs, labels = inputs.to(get_device()), labels.to(get_device())  # move data to device\n",
    "        classification_output = kp_model(inputs) # get results for the classification \n",
    "        \n",
    "        # Use this for classification\n",
    "        loss = criterion(classification_output, labels)\n",
    "        \n",
    "        # Use this for scoring\n",
    "        # loss = criterion(classification_output.float(), labels.float())\n",
    "        \n",
    "        total_loss += loss.item()  # accumulate the loss\n",
    "        _, predicted = torch.max(classification_output.data, 1)\n",
    "        # collect the predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    # calculate the validation metrics:\n",
    "    avg_loss = total_loss / len(test_loader)  # get the average loss\n",
    "    accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
