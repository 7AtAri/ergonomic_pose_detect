{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main File for all Models\n",
    "In this file we will train all the models with the best hyperparameters already found through the HPO notebooks that used optuna gridsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Imports and Helper Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmpose.apis import MMPoseInferencer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_cv2_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor to an OpenCV image.\n",
    "    \n",
    "    Parameters:\n",
    "    - tensor: A PyTorch tensor, in the format CxHxW with values normalized to [0, 1].\n",
    "    \n",
    "    Returns:\n",
    "    - An OpenCV image, in BGR format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Denormalize the tensor and convert it to a numpy array\n",
    "    image = tensor.mul(255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def visualize_output(image_tensor, keypoints_tensor, tup = [], show_neck_angle = False, show_trunk_angle = False, show_upper_arm_angle = False, show_lower_arm_angle = False):\n",
    "    \"\"\"\n",
    "    Visualize the output model on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor (torch.Tensor): The original image.\n",
    "    keypoints_tensor (torch.Tensor): Array of keypoints for pose estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    result_image = tensor_to_cv2_image(image_tensor)\n",
    "    keypoints = keypoints_tensor.cpu().numpy()\n",
    "    i = 0\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for x, y in keypoints:\n",
    "        color = (255, 0, 0) if i in tup else (0, 255, 0)            \n",
    "        i += 1\n",
    "        \n",
    "        cv2.circle(result_image, (int(x), int(y)), 20, color, -1)\n",
    "    \n",
    "    \n",
    "    # Helper points\n",
    "    hip_midpoint = keypoints[11] + ((keypoints[12] - keypoints[11]) / 2)\n",
    "    shoulder_midpoint = keypoints[5] + ((keypoints[6] - keypoints[5]) / 2)\n",
    "    nose_point = keypoints[0]\n",
    "    \n",
    "    # Draw lines\n",
    "    color = (255, 255, 191)\n",
    "    cyan = (0, 255, 255)\n",
    "    \n",
    "    if show_neck_angle:\n",
    "        # Mid shoulders - Nose\n",
    "        cv2.line(result_image, (int(shoulder_midpoint[0]), int(shoulder_midpoint[1])), (int(nose_point[0]), int(nose_point[1])), cyan, 10)\n",
    "        # Midpoint hip - Midpoint shoulders\n",
    "        cv2.line(result_image, (int(hip_midpoint[0]), int(hip_midpoint[1])), (int(shoulder_midpoint[0]), int(shoulder_midpoint[1])), cyan, 10)\n",
    "        # Midpoint hip blue\n",
    "        cv2.circle(result_image, (int(hip_midpoint[0]), int(hip_midpoint[1])), 20, (0, 0, 255), -1)\n",
    "        # Midpoint shoulders red\n",
    "        cv2.circle(result_image, (int(shoulder_midpoint[0]), int(shoulder_midpoint[1])), 20, (255, 0, 0), -1)\n",
    "        # Nose blue\n",
    "        cv2.circle(result_image, (int(nose_point[0]), int(nose_point[1])), 20, (0, 0, 255), -1)\n",
    "    if show_trunk_angle:\n",
    "        # Left - Right Hips\n",
    "        cv2.line(result_image, (int(keypoints[11][0]), int(keypoints[11][1])), (int(keypoints[12][0]), int(keypoints[12][1])), cyan, 10)\n",
    "        # Left Hip - Left Shoulder\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[11][0]), int(keypoints[11][1])), cyan, 10)\n",
    "        # Right Hip blue\n",
    "        cv2.circle(result_image, (int(keypoints[12][0]), int(keypoints[12][1])), 20, (0, 0, 255), -1)\n",
    "        # Left Hip red\n",
    "        cv2.circle(result_image, (int(keypoints[11][0]), int(keypoints[11][1])), 20, (255, 0, 0), -1)\n",
    "        # Left Shoulder blue\n",
    "        cv2.circle(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), 20, (0, 0, 255), -1)\n",
    "    if show_upper_arm_angle:\n",
    "        # Left Hip - Left Shoulders\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[11][0]), int(keypoints[11][1])), cyan, 10)\n",
    "        # Left Shoulder - Left Elbow\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[7][0]), int(keypoints[7][1])), cyan, 10)\n",
    "        # Left Hip blue\n",
    "        cv2.circle(result_image, (int(keypoints[11][0]), int(keypoints[11][1])), 20, (0, 0, 255), -1)\n",
    "        # Left Shoulder red\n",
    "        cv2.circle(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), 20, (255, 0, 0), -1)\n",
    "        # Left Elbow blue\n",
    "        cv2.circle(result_image, (int(keypoints[7][0]), int(keypoints[7][1])), 20, (0, 0, 255), -1)\n",
    "    if show_lower_arm_angle:\n",
    "        # Left Shoulder - Left Elbow\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[7][0]), int(keypoints[7][1])), cyan, 10)\n",
    "        # Left Elbow - Left Wrist\n",
    "        cv2.line(result_image, (int(keypoints[7][0]), int(keypoints[7][1])), (int(keypoints[9][0]), int(keypoints[9][1])), cyan, 10)\n",
    "        # Left Shoulder blue\n",
    "        cv2.circle(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), 20, (0, 0, 255), -1)\n",
    "        # Left Elbow red\n",
    "        cv2.circle(result_image, (int(keypoints[7][0]), int(keypoints[7][1])), 20, (255, 0, 0), -1)\n",
    "        # Left Wrist blue\n",
    "        cv2.circle(result_image, (int(keypoints[9][0]), int(keypoints[9][1])), 20, (0, 0, 255), -1)\n",
    "        \n",
    "        \n",
    "    if not (show_neck_angle or show_trunk_angle or show_upper_arm_angle or show_lower_arm_angle):\n",
    "        # Left - Right Shoulders\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[6][0]), int(keypoints[6][1])), color, 10)\n",
    "        # Left - Right Hips\n",
    "        cv2.line(result_image, (int(keypoints[11][0]), int(keypoints[11][1])), (int(keypoints[12][0]), int(keypoints[12][1])), color, 10)\n",
    "        # Left Shoulder - Left Hip\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[11][0]), int(keypoints[11][1])), color, 10)\n",
    "        # Right Shoulder - Right Hip\n",
    "        cv2.line(result_image, (int(keypoints[6][0]), int(keypoints[6][1])), (int(keypoints[12][0]), int(keypoints[12][1])), color, 10)\n",
    "        \n",
    "        # Mid shoulders - Nose\n",
    "        cv2.line(result_image, (int(shoulder_midpoint[0]), int(shoulder_midpoint[1])), (int(nose_point[0]), int(nose_point[1])), color, 10)\n",
    "        \n",
    "        # Left shoulder - Left elbow\n",
    "        cv2.line(result_image, (int(keypoints[5][0]), int(keypoints[5][1])), (int(keypoints[7][0]), int(keypoints[7][1])), color, 10)\n",
    "        # Right shoulder - Right elbow\n",
    "        cv2.line(result_image, (int(keypoints[6][0]), int(keypoints[6][1])), (int(keypoints[8][0]), int(keypoints[8][1])), color, 10)\n",
    "        # Left elbow - Left wrist\n",
    "        cv2.line(result_image, (int(keypoints[7][0]), int(keypoints[7][1])), (int(keypoints[9][0]), int(keypoints[9][1])), color, 10)\n",
    "        # Right elbow - Right wrist\n",
    "        cv2.line(result_image, (int(keypoints[8][0]), int(keypoints[8][1])), (int(keypoints[10][0]), int(keypoints[10][1])), color, 10)\n",
    "    \n",
    "    \n",
    "    return result_image\n",
    "\n",
    "def plot_results(image, mode=\"inline\", scale=1):\n",
    "    \"\"\"\n",
    "    Displays an image using either a popup window or inline display.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy.ndarray or CV2 image\n",
    "        The image to display.\n",
    "    mode : str, optional\n",
    "        The display mode. Must be either \"popup\" or \"inline\". If \"popup\", the image is displayed in a popup window. If \"inline\", the image is displayed inline in the notebook. Default is \"inline\".\n",
    "    scale : float, optional\n",
    "        The scaling factor for the image. Default is 1.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the mode is not \"popup\" or \"inline\".\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(image) is np.ndarray:\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    if mode == \"popup\":\n",
    "        pass\n",
    "    elif mode == \"inline\":\n",
    "        if type(image) == np.ndarray:\n",
    "            # Convert from BGR to RGB (because OpenCV uses BGR order for color channels, whereas PIL uses RGB.)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image_rgb)\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be either 'popup' or 'inline'\")\n",
    "\n",
    "    if type(image) is np.ndarray:\n",
    "        cv2.imshow('Image', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        width, height = image.size\n",
    "        display(image.resize((int(width*scale), int(height*scale))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images with binned labels (green, yellow, red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 118\n",
       "    Root location: E:/Users/Vipin/Documents/BHT/3. Semester/Learning from images/Pose Dataset_binned\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(640, 480), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = 'E:/Users/Vipin/Documents/BHT/3. Semester/Learning from images/Pose Dataset_binned'\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((640, 480)), # Resize images to 640x640\n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "dataset_binned = datasets.ImageFolder(root=data_path, transform=data_transforms)\n",
    "display(dataset_binned)\n",
    "\n",
    "seed_generator = torch.Generator().manual_seed(13)\n",
    "trainset_binned, testset_binned = random_split(dataset_binned, [0.8, 0.2], generator=seed_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images with the scores as from RULA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 118\n",
       "    Root location: E:/Users/Vipin/Documents/BHT/3. Semester/Learning from images/Pose Dataset Complete\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(640, 480), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = 'E:/Users/Vipin/Documents/BHT/3. Semester/Learning from images/Pose Dataset Complete'\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((640, 480)), # Resize images to 640x640\n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "dataset_scores = datasets.ImageFolder(root=data_path, transform=data_transforms)\n",
    "display(dataset_scores)\n",
    "\n",
    "seed_generator = torch.Generator().manual_seed(13)\n",
    "trainset_scores, testset_scores = random_split(dataset_scores, [0.8, 0.2], generator=seed_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "We are using the following pretrained models:\n",
    "- YOLOv8 Pose Estimation for keypoint detection\n",
    "- RTMPose (through mmpose) for keypoint detection\n",
    "\n",
    "For these models we take these approaches:\n",
    "- Using the keypoints as input for our models\n",
    "- Using the keypoints and calculating angles between the keypoints that are useful for pose quality estimation\n",
    "- Removing the last layers of the pretrained model and adding our own layer for the quality estimation instead of keypoint detection\n",
    "\n",
    "For the quality estimation we use two types of models:\n",
    "- ScoringModel that predicts the pose quality scores (1-7, given by us through the RULA sheet) on a continuous scale\n",
    "- ClassificationModel that predicts the pose quality class (green, yellow, red) based on the scores\n",
    "\n",
    "The binning for the classification is as follows:\n",
    "- Green: 1, 2\n",
    "- Yellow: 3, 4, 5\n",
    "- Red: 6, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8 for Keypoint Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTMPose for Keypoint Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8 for Keypoint Detection with our calculated angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
